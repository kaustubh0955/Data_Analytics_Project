{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7532120,"sourceType":"datasetVersion","datasetId":4386952}],"dockerImageVersionId":30635,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.style.use('ggplot')\n\nimport nltk","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-02-19T09:19:34.003636Z","iopub.execute_input":"2024-02-19T09:19:34.004075Z","iopub.status.idle":"2024-02-19T09:19:34.010747Z","shell.execute_reply.started":"2024-02-19T09:19:34.004036Z","shell.execute_reply":"2024-02-19T09:19:34.009578Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"df = pd.read_excel('/kaggle/input/pontiaa/google-pontiac.xlsx')\nprint(df.shape)\ndf = df.head(500)\nprint(df.shape)","metadata":{"execution":{"iopub.status.busy":"2024-02-19T09:19:34.013266Z","iopub.execute_input":"2024-02-19T09:19:34.013844Z","iopub.status.idle":"2024-02-19T09:19:34.159638Z","shell.execute_reply.started":"2024-02-19T09:19:34.013613Z","shell.execute_reply":"2024-02-19T09:19:34.158188Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"(657, 6)\n(500, 6)\n","output_type":"stream"}]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2024-02-19T09:19:34.161305Z","iopub.execute_input":"2024-02-19T09:19:34.161998Z","iopub.status.idle":"2024-02-19T09:19:34.177291Z","shell.execute_reply.started":"2024-02-19T09:19:34.161965Z","shell.execute_reply":"2024-02-19T09:19:34.175474Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"   Unnamed: 0           Author                Date  Rating  Number  \\\n0         NaN   Sanford Hejduk 2005-07-08 00:00:00       5       0   \n1         NaN     daddariotire 2017-09-15 16:12:14       1       5   \n2         NaN    Carmen J Peña 2018-03-28 23:20:56       5       1   \n3         NaN  William McGrath 2018-10-22 15:58:05       1       3   \n4         NaN        Ann Ellis 2019-01-07 14:48:13       5       0   \n\n                                             Reviews  \n0  I am a DSM @ location 085. This was a role pla...  \n1  I would not recommend Unifirst to anyone.Horri...  \n2                   Love my job at National Accounts  \n3  Decided we no longer needed their services at ...  \n4  Great company to work for. You have the abilit...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>Author</th>\n      <th>Date</th>\n      <th>Rating</th>\n      <th>Number</th>\n      <th>Reviews</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>NaN</td>\n      <td>Sanford Hejduk</td>\n      <td>2005-07-08 00:00:00</td>\n      <td>5</td>\n      <td>0</td>\n      <td>I am a DSM @ location 085. This was a role pla...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>NaN</td>\n      <td>daddariotire</td>\n      <td>2017-09-15 16:12:14</td>\n      <td>1</td>\n      <td>5</td>\n      <td>I would not recommend Unifirst to anyone.Horri...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>NaN</td>\n      <td>Carmen J Peña</td>\n      <td>2018-03-28 23:20:56</td>\n      <td>5</td>\n      <td>1</td>\n      <td>Love my job at National Accounts</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>NaN</td>\n      <td>William McGrath</td>\n      <td>2018-10-22 15:58:05</td>\n      <td>1</td>\n      <td>3</td>\n      <td>Decided we no longer needed their services at ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>NaN</td>\n      <td>Ann Ellis</td>\n      <td>2019-01-07 14:48:13</td>\n      <td>5</td>\n      <td>0</td>\n      <td>Great company to work for. You have the abilit...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"from nltk.sentiment import SentimentIntensityAnalyzer\nfrom tqdm.notebook import tqdm\n\nsia = SentimentIntensityAnalyzer()","metadata":{"execution":{"iopub.status.busy":"2024-02-19T09:19:34.179357Z","iopub.execute_input":"2024-02-19T09:19:34.180689Z","iopub.status.idle":"2024-02-19T09:19:34.201319Z","shell.execute_reply.started":"2024-02-19T09:19:34.180638Z","shell.execute_reply":"2024-02-19T09:19:34.199984Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"sia","metadata":{"execution":{"iopub.status.busy":"2024-02-19T09:19:34.204396Z","iopub.execute_input":"2024-02-19T09:19:34.205766Z","iopub.status.idle":"2024-02-19T09:19:34.216332Z","shell.execute_reply.started":"2024-02-19T09:19:34.205715Z","shell.execute_reply":"2024-02-19T09:19:34.214370Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"<nltk.sentiment.vader.SentimentIntensityAnalyzer at 0x7fdf348bd9f0>"},"metadata":{}}]},{"cell_type":"code","source":"import nltk  # Assuming you've already installed NLTK and downloaded the VADER lexicon\nfrom tqdm import tqdm  # Assuming you have tqdm installed\n\nsia = nltk.sentiment.vader.SentimentIntensityAnalyzer()\n\nres = {}\nfor i, row in tqdm(df.iterrows(), total=len(df)):\n    text = row['Reviews']\n    myid = row['Author']\n\n    # Ensure text is a string before sentiment analysis\n    if not isinstance(text, str):\n        text = str(text)\n\n    res[myid] = sia.polarity_scores(text)","metadata":{"execution":{"iopub.status.busy":"2024-02-19T09:19:34.218455Z","iopub.execute_input":"2024-02-19T09:19:34.218951Z","iopub.status.idle":"2024-02-19T09:19:34.610879Z","shell.execute_reply.started":"2024-02-19T09:19:34.218909Z","shell.execute_reply":"2024-02-19T09:19:34.609428Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"100%|██████████| 500/500 [00:00<00:00, 1356.09it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n\n# Load your dataset with reviews and labels (1 for good service, 0 for not)\n# df = pd.read_csv('reviews.csv')\n\n# Preprocessing\n# Clean the text, tokenize, and remove stopwords\ndef preprocess_text(text):\n    # Implement your text preprocessing steps here\n    return text\n\ndf['Processed Text'] = df['Reviews'].apply(preprocess_text)\n\n# Feature Extraction\n# Convert text data into numerical feature vectors using TF-IDF\ntfidf_vectorizer = TfidfVectorizer(stop_words='english')\nX = tfidf_vectorizer.fit_transform(df['Processed Text'])\ny = df['Rating']\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Model Training\n# Train a logistic regression model\nmodel = LogisticRegression()\nmodel.fit(X_train, y_train)\n\n# Model Evaluation\n# Predict on the test set and evaluate the model\ny_pred = model.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Accuracy:\", accuracy)\n\n# Keyword Extraction\n# Get feature names (words) from the TF-IDF vectorizer\nfeature_names = tfidf_vectorizer.get_feature_names_out()\n# Get coefficients from the trained logistic regression model\ncoefficients = model.coef_[0]\n\n# Get top keywords contributing to the positive class (good service)\ntop_keywords_indices = coefficients.argsort()[-10:]\ntop_keywords = [feature_names[idx] for idx in top_keywords_indices]\nprint(\"Top Keywords for Good Service:\", top_keywords)\n\n# Prediction\n# Example: Predict whether a new review indicates good service\nnew_review = \"The service was excellent and the staff was very helpful.\"\nnew_review_vectorized = tfidf_vectorizer.transform([new_review])\nprediction = model.predict(new_review_vectorized)\nif prediction[0] == 1:\n    print(\"The review indicates good service.\")\nelse:\n    print(\"The review does not indicate good service.\")\n","metadata":{"execution":{"iopub.status.busy":"2024-02-19T09:19:34.612924Z","iopub.execute_input":"2024-02-19T09:19:34.613280Z","iopub.status.idle":"2024-02-19T09:19:34.715068Z","shell.execute_reply.started":"2024-02-19T09:19:34.613249Z","shell.execute_reply":"2024-02-19T09:19:34.711132Z"},"trusted":true},"execution_count":15,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[15], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Feature Extraction\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Convert text data into numerical feature vectors using TF-IDF\u001b[39;00m\n\u001b[1;32m     21\u001b[0m tfidf_vectorizer \u001b[38;5;241m=\u001b[39m TfidfVectorizer(stop_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 22\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mtfidf_vectorizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mProcessed Text\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m y \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRating\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Split the data into training and testing sets\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:2133\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   2126\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params()\n\u001b[1;32m   2127\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf \u001b[38;5;241m=\u001b[39m TfidfTransformer(\n\u001b[1;32m   2128\u001b[0m     norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm,\n\u001b[1;32m   2129\u001b[0m     use_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_idf,\n\u001b[1;32m   2130\u001b[0m     smooth_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msmooth_idf,\n\u001b[1;32m   2131\u001b[0m     sublinear_tf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msublinear_tf,\n\u001b[1;32m   2132\u001b[0m )\n\u001b[0;32m-> 2133\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2134\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf\u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[1;32m   2135\u001b[0m \u001b[38;5;66;03m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[1;32m   2136\u001b[0m \u001b[38;5;66;03m# we set copy to False\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:1388\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1380\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1381\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1382\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1383\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1384\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1385\u001b[0m             )\n\u001b[1;32m   1386\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m-> 1388\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1390\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[1;32m   1391\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:1275\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1273\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m raw_documents:\n\u001b[1;32m   1274\u001b[0m     feature_counter \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m-> 1275\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m \u001b[43manalyze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1276\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1277\u001b[0m             feature_idx \u001b[38;5;241m=\u001b[39m vocabulary[feature]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:106\u001b[0m, in \u001b[0;36m_analyze\u001b[0;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Chain together an optional series of text processing steps to go from\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;124;03ma single document to ngrams, with or without tokenizing or preprocessing.\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;124;03m    A sequence of tokens, possibly with pairs, triples, etc.\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m decoder \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 106\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m analyzer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    108\u001b[0m     doc \u001b[38;5;241m=\u001b[39m analyzer(doc)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:239\u001b[0m, in \u001b[0;36m_VectorizerMixin.decode\u001b[0;34m(self, doc)\u001b[0m\n\u001b[1;32m    236\u001b[0m     doc \u001b[38;5;241m=\u001b[39m doc\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecode_error)\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m doc \u001b[38;5;129;01mis\u001b[39;00m np\u001b[38;5;241m.\u001b[39mnan:\n\u001b[0;32m--> 239\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    240\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnp.nan is an invalid document, expected byte or unicode string.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    241\u001b[0m     )\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m doc\n","\u001b[0;31mValueError\u001b[0m: np.nan is an invalid document, expected byte or unicode string."],"ename":"ValueError","evalue":"np.nan is an invalid document, expected byte or unicode string.","output_type":"error"}]},{"cell_type":"code","source":"# Filter reviews praising the drivers (assuming 'Processed Text' column contains preprocessed text)\npraising_driver_reviews = df[df['Reviews'].str.contains('driver')]\n\nif not praising_driver_reviews.empty:\n    # Convert filtered reviews into numerical feature vectors using TF-IDF\n    X_praise_drivers = tfidf_vectorizer.transform(praising_driver_reviews['Processed Text'])\n\n    # Predict sentiment using the logistic regression model\n    y_pred_praise_drivers = model.predict(X_praise_drivers)\n\n    # Extract keywords\n    if y_pred_praise_drivers.any() == 1:  # Check if any reviews predict positive sentiment\n        # Get coefficients from the trained logistic regression model\n        coefficients_praise_drivers = model.coef_[0]\n\n        # Get top keywords contributing to positive sentiment\n        top_keywords_indices_praise_drivers = coefficients_praise_drivers.argsort()[-10:]\n        top_keywords_praise_drivers = [feature_names[idx] for idx in top_keywords_indices_praise_drivers]\n        print(\"Top Keywords for Praising Drivers:\", top_keywords_praise_drivers)\n    else:\n        print(\"No reviews found praising the drivers.\")\nelse:\n    print(\"No reviews found mentioning drivers.\")\n","metadata":{"execution":{"iopub.status.busy":"2024-02-19T09:19:34.716431Z","iopub.status.idle":"2024-02-19T09:19:34.717126Z","shell.execute_reply.started":"2024-02-19T09:19:34.716721Z","shell.execute_reply":"2024-02-19T09:19:34.716806Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import spacy\n\n# Load spaCy's English language model\nnlp = spacy.load(\"en_core_web_sm\")\n\n# Example review praising someone\nreview = \"Fabian Santo is always a friendly encounter and he gets our concerns completed in a timely manner when they arise. Keep up the excellent work Fabian from the service team at Carmax in Attleboro.\"\n\n# Process the review using spaCy\ndoc = nlp(review)\n\n# Extract proper nouns (persons) from the review\npraised_persons = [entity.text for entity in doc.ents if entity.label_ == \"PERSON\"]\n\n# Print the extracted persons (potential keywords indicating praise)\nif praised_persons:\n    print(\"Persons praised in the review:\", praised_persons)\nelse:\n    print(\"No persons praised in the review.\")\n","metadata":{"execution":{"iopub.status.busy":"2024-02-19T09:19:34.718938Z","iopub.status.idle":"2024-02-19T09:19:34.719767Z","shell.execute_reply.started":"2024-02-19T09:19:34.719492Z","shell.execute_reply":"2024-02-19T09:19:34.719527Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import spacy\n\n# Load spaCy's English language model\nnlp = spacy.load(\"en_core_web_sm\")\n\n# Example review praising someone\nreview = \"Fabian Santo is always a friendly encounter and he gets our concerns completed in a timely manner when they arise. Keep up the excellent work Fabian from the service team at Carmax in Attleboro.\"\n\n# Process the review using spaCy\ndoc = nlp(review)\n\n# Extract adjectives from the review\nadjectives = [token.text for token in doc if token.pos_ == \"ADJ\"]\n\n# Print the extracted adjectives (potential keywords indicating praise)\nif adjectives:\n    print(\"Adjectives indicating praise in the review:\", adjectives)\nelse:\n    print(\"No adjectives indicating praise in the review.\")\n","metadata":{"execution":{"iopub.status.busy":"2024-02-19T09:19:34.723459Z","iopub.status.idle":"2024-02-19T09:19:34.724173Z","shell.execute_reply.started":"2024-02-19T09:19:34.723947Z","shell.execute_reply":"2024-02-19T09:19:34.723977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import spacy\nimport pandas as pd\n\n# Load spaCy's English language model\nnlp = spacy.load(\"en_core_web_sm\")\n\n# Example DataFrame containing reviews\n# Replace this with your actual DataFrame containing reviews\ndf = pd.DataFrame({\n    'Reviews': [\n        \"Fabian Santo is always a friendly encounter and he gets our concerns completed in a timely manner when they arise. Keep up the excellent work Fabian from the service team at Carmax in Attleboro.\",\n        \"The service provided by the team was exceptional. They were very professional and helpful.\",\n        \"I had a wonderful experience with the service staff. They were courteous and efficient.\",\n        \"The staff went above and beyond to assist me. I'm extremely satisfied with their service.\",\n        \"The service was top-notch. The staff was attentive and friendly.\"\n    ]\n})\n\n# Function to extract adjectives indicating praise from a review\ndef extract_praise_adjectives(review):\n    # Process the review using spaCy\n    doc = nlp(review)\n    # Extract adjectives from the review\n    adjectives = [token.text for token in doc if token.pos_ == \"ADJ\"]\n    return adjectives\n\n# Apply the function to each review in the DataFrame\ndf['Praise Adjectives'] = df['Reviews'].apply(extract_praise_adjectives)\n\n# Print the DataFrame with extracted adjectives\nprint(df)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-19T09:19:34.725337Z","iopub.status.idle":"2024-02-19T09:19:34.726004Z","shell.execute_reply.started":"2024-02-19T09:19:34.725793Z","shell.execute_reply":"2024-02-19T09:19:34.725821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import spacy\nimport pandas as pd\n\n# Load spaCy's English language model\nnlp = spacy.load(\"en_core_web_sm\")\n\n# Function to extract adjectives indicating praise from a review\ndef extract_praise_adjectives(review):\n    if pd.isna(review):  # Check for NaN values\n        return []\n    # Process the review using spaCy\n    doc = nlp(review)\n    # Extract adjectives from the review\n    adjectives = [token.text for token in doc if token.pos_ == \"ADJ\"]\n    return adjectives\n\n# Read reviews from an Excel sheet\ndf = pd.read_excel(\"/kaggle/input/pontiaa/google-pontiac.xlsx\")\n\n# Apply the function to each review in the DataFrame\ndf['Praise Adjectives'] = df['Reviews'].apply(extract_praise_adjectives)\n\n# Print the DataFrame with extracted adjectives\nprint(df)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-19T09:19:34.727145Z","iopub.status.idle":"2024-02-19T09:19:34.727779Z","shell.execute_reply.started":"2024-02-19T09:19:34.727574Z","shell.execute_reply":"2024-02-19T09:19:34.727596Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Generate a table with all reviews\nall_reviews_table = df[['Author', 'Date', 'Rating', 'Reviews', 'Praise Adjectives']]\n\n# Display the table\nprint(all_reviews_table)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-19T09:19:34.728901Z","iopub.status.idle":"2024-02-19T09:19:34.729339Z","shell.execute_reply.started":"2024-02-19T09:19:34.729148Z","shell.execute_reply":"2024-02-19T09:19:34.729167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nimport spacy\n\n# Load spaCy's English language model\nnlp = spacy.load(\"en_core_web_sm\")\n\n# Assuming df is your DataFrame containing reviews and 'Processed Text' column contains preprocessed text\n# Filter reviews mentioning drivers\npraising_driver_reviews = df[df['Reviews'].str.contains('driver', case=False)]\n\nif not praising_driver_reviews.empty:\n    # Extract persons (names) praised in the reviews\n    praised_persons = []\n    for review in praising_driver_reviews['Reviews']:\n        doc = nlp(review)\n        persons = [entity.text for entity in doc.ents if entity.label_ == \"PERSON\"]\n        praised_persons.extend(persons)\n\n    # Convert persons list to lowercase for consistency\n    praised_persons = [person.lower() for person in praised_persons]\n\n    # Filter out duplicates\n    praised_persons = list(set(praised_persons))\n\n    if praised_persons:\n        print(\"Persons praised in reviews mentioning drivers:\", praised_persons)\n    else:\n        print(\"No persons praised in reviews mentioning drivers.\")\nelse:\n    print(\"No reviews found mentioning drivers.\")\n","metadata":{"execution":{"iopub.status.busy":"2024-02-19T09:19:34.731288Z","iopub.status.idle":"2024-02-19T09:19:34.731753Z","shell.execute_reply.started":"2024-02-19T09:19:34.731545Z","shell.execute_reply":"2024-02-19T09:19:34.731564Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\n\n# Load the dataset\n# Assuming df is your DataFrame containing the reviews and ratings\n\n# Feature Extraction\n# Convert text data into numerical feature vectors using TF-IDF\ntfidf_vectorizer = TfidfVectorizer(stop_words='english')\nX = tfidf_vectorizer.fit_transform(df['Reviews'])\ny = df['Rating']\n\n# Model Training\n# Train a logistic regression model\nmodel = LogisticRegression()\nmodel.fit(X, y)\n\n# Get the vocabulary (feature names) from the TF-IDF vectorizer\nfeature_names = tfidf_vectorizer.get_feature_names_out()\n\n# Get coefficients from the trained logistic regression model\ncoefficients = model.coef_[0]\n\n# Get top keywords contributing to positive sentiment (rating 5)\ntop_keywords_indices = coefficients.argsort()[-10:]\ntop_keywords = [feature_names[idx] for idx in top_keywords_indices]\nprint(\"Top Keywords for Positive Sentiment (Rating 5):\", top_keywords)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-19T09:19:34.733201Z","iopub.status.idle":"2024-02-19T09:19:34.733622Z","shell.execute_reply.started":"2024-02-19T09:19:34.733436Z","shell.execute_reply":"2024-02-19T09:19:34.733455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Predict sentiment for all reviews\ny_pred = model.predict(X)\n\n# Count the number of reviews classified as positive sentiment (assuming rating 5 indicates positive sentiment)\npositive_reviews_count = sum(y_pred == 5)\n\nprint(\"Number of reviews based on good service (rating 5):\", positive_reviews_count)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-19T09:19:34.735000Z","iopub.status.idle":"2024-02-19T09:19:34.735371Z","shell.execute_reply.started":"2024-02-19T09:19:34.735197Z","shell.execute_reply":"2024-02-19T09:19:34.735214Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import nltk\nfrom tqdm import tqdm\nimport pandas as pd\n\nvaders=pd.DataFrame(res).T\nvaders = vaders.reset_index().rename(columns={'index': 'CustomerName'})\n\n# Assuming you've already installed NLTK and downloaded the VADER lexicon\nsia = nltk.sentiment.vader.SentimentIntensityAnalyzer()\n\nres = {}\n\n# Assuming 'wiI7pd' is the column containing text and 'd4r55' is the column containing years\nfor i, row in tqdm(df.iterrows(), total=len(df)):\n    text = row['Reviews']\n    myid = row['Author']\n    year = row['Date']  # Assuming 'year' is the column containing the year information\n\n    # Ensure text is a string before sentiment analysis\n    if not isinstance(text, str):\n        text = str(text)\n\n    res[myid] = {'Date': year, **sia.polarity_scores(text)}\n\n# Creating a DataFrame from the sentiment scores\nvaders = pd.DataFrame(res).T.reset_index().rename(columns={'index': 'CustomerName'})\n# Sorting the DataFrame based on the 'year' column in descending order\nvaders_sorted = vaders.sort_values(by='Date', ascending=False)\n\n# Displaying the sorted DataFrame\nvaders_sorted.head()","metadata":{"execution":{"iopub.status.busy":"2024-02-19T09:19:34.737246Z","iopub.status.idle":"2024-02-19T09:19:34.737684Z","shell.execute_reply.started":"2024-02-19T09:19:34.737491Z","shell.execute_reply":"2024-02-19T09:19:34.737510Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import nltk\nfrom tqdm import tqdm\nimport pandas as pd\n\n\nvaders=pd.DataFrame(res).T\nvaders = vaders.reset_index().rename(columns={'index': 'CustomerName'})\n# Assuming you've already installed NLTK and downloaded the VADER lexicon\nsia = nltk.sentiment.vader.SentimentIntensityAnalyzer()\n\n# Function to calculate sentiment scores\ndef calculate_sentiment(text):\n    if not isinstance(text, str):\n        text = str(text)\n    return sia.polarity_scores(text)\n\n# Assuming 'wiI7pd' is the column containing text, 'd4r55' is the column containing years, and 'year' is the column containing the year information\ndf['sentiment_scores'] = df['Reviews'].apply(calculate_sentiment)\n\n# Extracting 'year' from the sentiment scores\ndf['Date'] = df['Date']  # Assuming 'year' is the column containing the year information\n\n# Sorting the DataFrame based on the 'year' column in descending order\nsorted_df = df.sort_values(by='Date', ascending=False)\n\n# Displaying the sorted DataFrame\nsorted_df[['Reviews', 'Author', 'Date']].head()","metadata":{"execution":{"iopub.status.busy":"2024-02-19T09:19:34.739178Z","iopub.status.idle":"2024-02-19T09:19:34.739976Z","shell.execute_reply.started":"2024-02-19T09:19:34.739736Z","shell.execute_reply":"2024-02-19T09:19:34.739764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"positive_counts_by_year = df[df['sentiment_scores'].apply(lambda x: x['pos'] > x['neg'])].groupby('Date').size()\n\n\nprint(\"Positive Review Counts by Year:\")\nprint(positive_counts_by_year)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-19T09:19:34.741632Z","iopub.status.idle":"2024-02-19T09:19:34.742023Z","shell.execute_reply.started":"2024-02-19T09:19:34.741838Z","shell.execute_reply":"2024-02-19T09:19:34.741857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Assuming you have already computed the DataFrame df with sentiment scores and year information\n\n# Grouping the DataFrame by year and counting the number of negative reviews in each year\nnegative_counts_by_year = df[df['sentiment_scores'].apply(lambda x: x['neg'] > x['pos'])].groupby('Date').size()\n\n# Printing the total number of negative reviews in each year\nprint(\"Negative Review Counts by Year:\")\nprint(negative_counts_by_year)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-19T09:19:34.743746Z","iopub.status.idle":"2024-02-19T09:19:34.745046Z","shell.execute_reply.started":"2024-02-19T09:19:34.744834Z","shell.execute_reply":"2024-02-19T09:19:34.744856Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"neutral_counts_by_year = df[df['sentiment_scores'].apply(lambda x: x['pos'] == x['neg'])].groupby('Date').size()\n\nprint(\"Neutral Review Counts by Year:\")\nprint(neutral_counts_by_year)","metadata":{"execution":{"iopub.status.busy":"2024-02-19T09:19:34.746446Z","iopub.status.idle":"2024-02-19T09:19:34.746836Z","shell.execute_reply.started":"2024-02-19T09:19:34.746657Z","shell.execute_reply":"2024-02-19T09:19:34.746675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import nltk\nimport pandas as pd\n\n# Assuming you've already installed NLTK and downloaded the VADER lexicon\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\n\n# Initialize the VADER Sentiment Analyzer\nsia = SentimentIntensityAnalyzer()\n\n# Function to calculate sentiment scores\ndef calculate_sentiment(text):\n    if not isinstance(text, str):\n        text = str(text)\n    return sia.polarity_scores(text)\n\n# Assuming 'wiI7pd' is the column containing text, and 'year' is the column containing the year information\n# You need to adjust the column names as per your DataFrame\ndf['sentiment_scores'] = df['Reviews'].apply(calculate_sentiment)\n\n# Extracting 'year' from the sentiment scores\ndf['Date'] = df['Date'].dt.year  # Extract only the year from the datetime column\n\n# Counting the number of reviews for each year\nreview_counts_by_year = df['Date'].value_counts()\n\n# Sorting the years in descending order and obtaining the corresponding review counts\nsorted_years = review_counts_by_year.sort_index(ascending=False)\n\n# Splitting the DataFrame into positive, neutral, and negative reviews\npositive_reviews = df[df['sentiment_scores'].apply(lambda x: x['pos'] > x['neg'])]\nneutral_reviews = df[df['sentiment_scores'].apply(lambda x: x['pos'] == x['neg'])]\nnegative_reviews = df[df['sentiment_scores'].apply(lambda x: x['neg'] > x['pos'])]\n\n# Counting the number of positive, neutral, and negative reviews for each year\npositive_review_counts_by_year = positive_reviews['Date'].value_counts().reindex(sorted_years.index, fill_value=0)\nneutral_review_counts_by_year = neutral_reviews['Date'].value_counts().reindex(sorted_years.index, fill_value=0)\nnegative_review_counts_by_year = negative_reviews['Date'].value_counts().reindex(sorted_years.index, fill_value=0)\n\n# Creating a DataFrame to hold the review counts by year and sentiment\nreview_counts_df = pd.DataFrame({\n    'Year': sorted_years.index,\n    'Total Reviews': sorted_years.values,\n    'Positive Reviews': positive_review_counts_by_year.values,\n    'Neutral Reviews': neutral_review_counts_by_year.values,\n    'Negative Reviews': negative_review_counts_by_year.values\n})\n\n# Displaying the DataFrame\nprint(review_counts_df)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-19T09:19:34.748216Z","iopub.status.idle":"2024-02-19T09:19:34.748617Z","shell.execute_reply.started":"2024-02-19T09:19:34.748435Z","shell.execute_reply":"2024-02-19T09:19:34.748454Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import nltk\nimport pandas as pd\n\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\n\n\nsia = SentimentIntensityAnalyzer()\n\n\ndef calculate_sentiment(text):\n    if not isinstance(text, str):\n        text = str(text)\n    return sia.polarity_scores(text)\n\n\ndef categorize_sentiment(Rating):\n    if Rating > 4:\n        return 'Positive'\n    elif 2 < Rating <= 3:\n        return 'Neutral'\n    else:\n        return 'Negative'\n\n\ndf['sentiment_scores'] = df['Reviews'].apply(calculate_sentiment)\ndf['Sentiment'] = df['Rating'].apply(categorize_sentiment)\n\n\ndf['Date'] = df['Date'].dt.year  \n\nreview_counts_by_year = df['Date'].value_counts()\n\n\nsorted_years = review_counts_by_year.sort_index(ascending=False)\n\n\npositive_reviews = df[df['Sentiment'] == 'Positive']\nneutral_reviews = df[df['Sentiment'] == 'Neutral']\nnegative_reviews = df[df['Sentiment'] == 'Negative']\n\npositive_review_counts_by_year = positive_reviews['Date'].value_counts().reindex(sorted_years.index, fill_value=0)\nneutral_review_counts_by_year = neutral_reviews['Date'].value_counts().reindex(sorted_years.index, fill_value=0)\nnegative_review_counts_by_year = negative_reviews['Date'].value_counts().reindex(sorted_years.index, fill_value=0)\n\n\nreview_counts_df = pd.DataFrame({\n    'Year': sorted_years.index,\n    'Total Reviews': sorted_years.values,\n    'Positive Reviews': positive_review_counts_by_year.values,\n    'Neutral Reviews': neutral_review_counts_by_year.values,\n    'Negative Reviews': negative_review_counts_by_year.values\n})\n\n# Displaying the DataFrame\nprint(review_counts_df)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-19T09:19:34.749775Z","iopub.status.idle":"2024-02-19T09:19:34.750605Z","shell.execute_reply.started":"2024-02-19T09:19:34.750334Z","shell.execute_reply":"2024-02-19T09:19:34.750356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nratings_counts_by_year = df.groupby(['Date', 'Rating']).size().unstack(fill_value=0)\nratings_counts_by_year = ratings_counts_by_year.reindex(columns=range(1, 6), fill_value=0)\n\nprint(ratings_counts_by_year)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-19T09:19:34.752556Z","iopub.status.idle":"2024-02-19T09:19:34.752930Z","shell.execute_reply.started":"2024-02-19T09:19:34.752754Z","shell.execute_reply":"2024-02-19T09:19:34.752771Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Plotting the bar chart\nplt.figure(figsize=(10, 6))\n\n# Width of each bar\nbar_width = 0.25\n\n# Positions for the bars\nyears = range(len(review_counts_df))\n\n# Plotting positive reviews\nplt.bar([year - bar_width for year in years], review_counts_df['Positive Reviews'], bar_width, label='Positive', color='lightgreen')\n\n# Plotting neutral reviews\nplt.bar(years, review_counts_df['Neutral Reviews'], bar_width, label='Neutral', color='lightblue')\n\n# Plotting negative reviews\nplt.bar([year + bar_width for year in years], review_counts_df['Negative Reviews'], bar_width, label='Negative', color='lightcoral')\n\nplt.title('Reviews by Year and Sentiment')\nplt.xlabel('Year')\nplt.ylabel('Number of Reviews')\nplt.xticks(years, review_counts_df['Year'], rotation=45)\nplt.legend(title='Sentiment', loc='upper left')\nplt.grid(axis='y', linestyle='--', alpha=0.7)\nplt.tight_layout()\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-02-19T09:19:34.754525Z","iopub.status.idle":"2024-02-19T09:19:34.755056Z","shell.execute_reply.started":"2024-02-19T09:19:34.754797Z","shell.execute_reply":"2024-02-19T09:19:34.754826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Assuming you have already computed the review counts DataFrame named 'review_counts_df'\n\n# Plotting the bar chart\nplt.figure(figsize=(10, 6))\n\n# Plotting positive reviews\nplt.bar(review_counts_df['Year'], review_counts_df['Positive Reviews'], label='Positive', color='lightgreen')\n\n# Plotting neutral reviews\nplt.bar(review_counts_df['Year'], review_counts_df['Neutral Reviews'], bottom=review_counts_df['Positive Reviews'], label='Neutral', color='lightblue')\n\n# Plotting negative reviews\nplt.bar(review_counts_df['Year'], review_counts_df['Negative Reviews'], bottom=review_counts_df['Positive Reviews'] + review_counts_df['Neutral Reviews'], label='Negative', color='lightcoral')\n\nplt.title('Reviews by Year and Sentiment')\nplt.xlabel('Year')\nplt.ylabel('Number of Reviews')\nplt.xticks(rotation=45)\nplt.legend(title='Sentiment', loc='upper left')\nplt.grid(axis='y', linestyle='--', alpha=0.7)\nplt.tight_layout()\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-02-19T09:19:34.756629Z","iopub.status.idle":"2024-02-19T09:19:34.757298Z","shell.execute_reply.started":"2024-02-19T09:19:34.757101Z","shell.execute_reply":"2024-02-19T09:19:34.757122Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Assuming you have already computed the review counts DataFrame named 'review_counts_df'\n\n# Calculate the total number of reviews for each year\nreview_counts_df['Total'] = review_counts_df['Positive Reviews'] + review_counts_df['Neutral Reviews'] + review_counts_df['Negative Reviews']\n\n# Calculate the percentage distribution of positive, neutral, and negative reviews for each year\nreview_counts_df['Positive (%)'] = (review_counts_df['Positive Reviews'] / review_counts_df['Total']) * 100\nreview_counts_df['Neutral (%)'] = (review_counts_df['Neutral Reviews'] / review_counts_df['Total']) * 100\nreview_counts_df['Negative (%)'] = (review_counts_df['Negative Reviews'] / review_counts_df['Total']) * 100\n\n# Plotting the pie chart for each year\nfor year in review_counts_df['Year']:\n    year_data = review_counts_df[review_counts_df['Year'] == year]\n    labels = ['Positive', 'Neutral', 'Negative']\n    sizes = [year_data.iloc[0]['Positive (%)'], year_data.iloc[0]['Neutral (%)'], year_data.iloc[0]['Negative (%)']]\n    \n    plt.figure(figsize=(6, 6))\n    plt.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=140)\n    plt.title(f'Review Distribution for Year {year}')\n    plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle\n    plt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-02-19T09:19:34.759112Z","iopub.status.idle":"2024-02-19T09:19:34.759536Z","shell.execute_reply.started":"2024-02-19T09:19:34.759320Z","shell.execute_reply":"2024-02-19T09:19:34.759338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Assuming you have already computed the review counts DataFrame named 'review_counts_df'\n\n# Plotting the scatter plot\nplt.figure(figsize=(10, 6))\nplt.scatter(review_counts_df['Year'], review_counts_df['Total Reviews'], color='blue', marker='o')\nplt.title('Total Reviews by Year')\nplt.xlabel('Year')\nplt.ylabel('Total Number of Reviews')\nplt.grid(True)\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-02-19T09:19:34.761151Z","iopub.status.idle":"2024-02-19T09:19:34.761566Z","shell.execute_reply.started":"2024-02-19T09:19:34.761359Z","shell.execute_reply":"2024-02-19T09:19:34.761391Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Assuming you have already computed the review counts DataFrame named 'review_counts_df'\n\n# Set up the seaborn style\nsns.set(style=\"white\", color_codes=True)\n\n# Create a pairplot with marginal histograms\ng = sns.pairplot(review_counts_df, \n                 x_vars=['Positive Reviews', 'Neutral Reviews', 'Negative Reviews'], \n                 y_vars=['Total Reviews'],\n                 height=5, aspect=1.5)\n\n# Set labels and titles\nplt.suptitle('Marginal Histograms for Positive, Neutral, and Negative Reviews vs Total Reviews', y=1.02)\ng.set_axis_labels('Number of Positive Reviews', 'Total Number of Reviews', fontsize=12)\n\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-02-19T09:19:34.763814Z","iopub.status.idle":"2024-02-19T09:19:34.764230Z","shell.execute_reply.started":"2024-02-19T09:19:34.764040Z","shell.execute_reply":"2024-02-19T09:19:34.764060Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Assuming you have already computed the review counts DataFrame named 'review_counts_df'\n\n# Compute the correlation matrix\ncorrelation_matrix = review_counts_df[['Positive Reviews', 'Neutral Reviews', 'Negative Reviews', 'Total Reviews']].corr()\n\n# Set up the seaborn style\nsns.set(style=\"white\")\n\n# Create the heatmap\nplt.figure(figsize=(8, 6))\nsns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\", linewidths=0.5)\nplt.title('Correlation Matrix of Review Counts')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-02-19T09:19:34.765363Z","iopub.status.idle":"2024-02-19T09:19:34.765869Z","shell.execute_reply.started":"2024-02-19T09:19:34.765686Z","shell.execute_reply":"2024-02-19T09:19:34.765707Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Assuming you have already computed the review counts DataFrame named 'review_counts_df'\n\n# Initialize the subplot\nfig, axes = plt.subplots(len(review_counts_df), 1, figsize=(6, 6 * len(review_counts_df)), sharex=True)\n\n# Plotting pie charts and creating tables\nfor i, (year, data) in enumerate(review_counts_df.iterrows()):\n    labels = ['Positive', 'Neutral', 'Negative']\n    sizes = [data['Positive (%)'], data['Neutral (%)'], data['Negative (%)']]\n    \n    # Plotting pie chart\n    ax = axes[i]\n    ax.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=140)\n    ax.set_title(f'Review Distribution for Year {year}')\n    ax.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle\n    \n    # Creating table\n    table_data = [[f'{size:.1f}%' for size in sizes]]\n    ax.table(cellText=table_data, colLabels=labels, loc='bottom', cellLoc='center')\n\nplt.tight_layout()\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-02-19T09:19:34.767430Z","iopub.status.idle":"2024-02-19T09:19:34.767815Z","shell.execute_reply.started":"2024-02-19T09:19:34.767636Z","shell.execute_reply":"2024-02-19T09:19:34.767654Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Assuming you have already computed the review counts DataFrame named 'review_counts_df'\n\n# Combine the percentage data into a single DataFrame\npercentage_data = review_counts_df[['Positive (%)', 'Neutral (%)', 'Negative (%)']]\n\n# Create the table\nfig, ax = plt.subplots(figsize=(10, 6))\ntable = ax.table(cellText=percentage_data.values,\n                 colLabels=percentage_data.columns,\n                 rowLabels=review_counts_df.index.astype(str),\n                 loc='center')\ntable.auto_set_font_size(False)\ntable.set_fontsize(10)\ntable.scale(1.2, 1.2)  # Adjust table size\n\n# Hide axes\nax.axis('off')\n\n# Set title\nax.set_title('Percentage Distribution of Reviews by Year')\n\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-02-19T09:19:34.769807Z","iopub.status.idle":"2024-02-19T09:19:34.770209Z","shell.execute_reply.started":"2024-02-19T09:19:34.770026Z","shell.execute_reply":"2024-02-19T09:19:34.770044Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Assuming you have already computed the review counts DataFrame named 'review_counts_df'\n\n# Set up the figure and axes\nfig, ax = plt.subplots(figsize=(10, 6))\n\n# Plotting the bar chart\nbar_width = 0.25\nindex = review_counts_df.index.astype(str)\nyears = range(len(index))\n\n# Plotting positive reviews\nax.bar(years, review_counts_df['Positive (%)'], bar_width, label='Positive', color='lightgreen')\n\n# Plotting neutral reviews\nax.bar([year + bar_width for year in years], review_counts_df['Neutral (%)'], bar_width, label='Neutral', color='lightblue')\n\n# Plotting negative reviews\nax.bar([year + 2*bar_width for year in years], review_counts_df['Negative (%)'], bar_width, label='Negative', color='lightcoral')\n\n# Set x-axis labels\nax.set_xticks([year + bar_width for year in years])\nax.set_xticklabels(index)\n\n# Set labels and title\nax.set_xlabel('Year')\nax.set_ylabel('Percentage of Reviews')\nax.set_title('Percentage Distribution of Reviews by Year')\nax.legend()\n\n# Show plot\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-02-19T09:19:34.771356Z","iopub.status.idle":"2024-02-19T09:19:34.771843Z","shell.execute_reply.started":"2024-02-19T09:19:34.771579Z","shell.execute_reply":"2024-02-19T09:19:34.771598Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}